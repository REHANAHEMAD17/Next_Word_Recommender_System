{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ec0c3-ba9a-47cb-a57a-e9d68914aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad783ff-31f2-4fbe-aebf-cd4f11a351c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b463c-2f1e-4011-aa30-5e4ffc597d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mounting the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2cd57-8905-4882-aa8d-446f90c3230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data\n",
    "with open ('/content/drive/MyDrive/NLP/dialogs_dataset', 'rb') as f:\n",
    "    dialogs= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767e6f7-f6f3-4f04-a5b0-ef7d5f1ab07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c87e6b-9bbd-41e0-b6cf-cb0208b21b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(dialogs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d60d1f-1253-4218-a7b4-20ca7e718e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "dialogs_clean= []\n",
    "for i in dialogs:\n",
    "    # remove everythings except alphabets, ' and whith spaces\n",
    "    re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
    "    # Convert text to lowercase\n",
    "    i= i.lower()\n",
    "    # add cleaned text to the file\n",
    "    dialogs_clean.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948988f0-18d2-48fe-896e-4d4b3d391afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(dialogs_clean, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b8448-36a9-4eec-b5eb-33694358d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocabulary\n",
    "# get list of all the words\n",
    "all_words= \" \".join(dialogs_clean).split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b6582-3b07-453f-856b-f43824bfccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict= {}\n",
    "# add word-count pair to the dictionary\n",
    "for word in all_words:\n",
    "    # check if the words is already in dictionary\n",
    "    if word in words_dict:\n",
    "        # Increment count of word by 1\n",
    "        words_dict[word] = words_dict[word] +1\n",
    "    else:\n",
    "        # add the word to dictioanry with count 1\n",
    "        words_dict[word]= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709b441-50ed-4ab9-be0f-31b2765ddebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a66213-e1b9-4438-b719-2d548479b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datafreame\n",
    "words_df= pd.DataFrame({'word' : list(words_dict.keys()), 'count': list(words_dict.values())})\n",
    "words_df.reset_index(inplace= True, drop = True)\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c6e5e-993e-4ed4-ba64-b172633f55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words with higest frequency\n",
    "words.df.tai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d8e02-33aa-4c31-90a1-03aa8d0d4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocalbulary_size\n",
    "len(words_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df8fab-a4a8-4619-9419-b58041215515",
   "metadata": {},
   "source": [
    "### 2. Creating N-grams of the dialoages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd75bf7-3c3d-4d24-9439-782ba96a5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.dataframe()\n",
    "# Adding the cleaned sentences in the dataframe\n",
    "dataset['sentences']= dialogs_clean\n",
    "\n",
    "# first 20 cleaned senteces\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5167a35-43c9-4f55-8fbd-a5f2e14f5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .split () to get tokens from the sentences\n",
    "dataset['Sentences'][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924bd85-b3fd-44b4-bcbf-b8da92aaf376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to create unigrams\n",
    "# taking a sentences as input\n",
    "def create_unigram(sentence):\n",
    "    # creating tokens from the sentence\n",
    "    tokens= sentence.split()\n",
    "    # empty list to stroe the unigrams\n",
    "    unigram_list= []\n",
    "    # no of the unigrams is equal to the no of the tokens in the sentences\n",
    "    for i in range(len(tokens)):\n",
    "        # appending the each unigram in the list\n",
    "        unigram_list.append(tokens[i : i+1])\n",
    "        #returing the unigram list for a senteces\n",
    "    return unigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35e337-7fc7-4ce8-89dc-eb3de81a4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to create bigrams\n",
    "def create_bigram(sentence):\n",
    "    tokens= sentence.split()\n",
    "    biagrams_list= []\n",
    "    # no of the bigrams is one less than the no of tokens in the senteces\n",
    "    for i in range(len(tokens) -1):\n",
    "        biagram_list.append(tokens[i: i+2])\n",
    "    return biagram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e192a-003e-475c-82a5-f38fbb0bcd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trigram(sentence):\n",
    "    tokens= sentence.split()\n",
    "    trigram_list=[]\n",
    "    # no of the trigrams is two less than the no of the tokens in the sentence\n",
    "    for i in range(len(token)- 2):\n",
    "        trigram_list.append(tokens[i : i +3])\n",
    "    return trigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b1934-75d5-44da-9cc4-83be20af268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the unigram for all the senteces in the dataset\n",
    "final_unigram=[]\n",
    "# for each sentence\n",
    "for i in range(dataset.shape[0]):\n",
    "    # using the defined unigram function to create unigrams\n",
    "    final_unigram.append(create_unigram(dataset['Sentences'][i]))\n",
    "    # Adding the unigram in a seperate column in the dataset\n",
    "    dataset['unigram']= final_unigram\n",
    "    # creating biargams for all the sentences in the dataset\n",
    "    final_bigram= []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        final_bigram.append(create_bigram(dataset['Sentences'][i]))\n",
    "    dataset['bigram']= final_bigram\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ee0d1-4981-449a-b43b-59106932e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating trigrams for all the senteces in the dataset\n",
    "final_trigrams= []\n",
    "for i in range(dataset.shape[0]):\n",
    "    final_trigram.append(create_trigram(dataset['Sentences'][i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755a30b-10ad-4b93-85ff-8edf5c4b5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['trigram']= final_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50885d0-72b1-415b-ac99-3645a5ac051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dc489-f6c0-4095-9d01-e8a9e7d2dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentences\n",
    "dataset['Sentences'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e2036-ecf2-4127-8209-c39fc9b97c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram of the sentece\n",
    "dataset['unigram'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d7f10-39af-4a67-8cc1-dcd88ddb9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram of the sentence\n",
    "dataset['bigram'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37515d8c-4fb3-44fb-87eb-2f985b563584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram of the sentence\n",
    "dataset['trigram'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58871e37-4228-4b4d-99d1-be5e7ca3c554",
   "metadata": {},
   "source": [
    "### 3.Building the N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49463a-3c77-41de-acf0-1e46838fac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr defining the N-gram model\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf95bc-f097-4cd8-be51-e1abea4ee511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a placeholder for model\n",
    "model = defaultdict(lambda : defaultdict(lambda : 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f96d0-d9c2-4d9f-a9e2-0b6b51b6ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequ of co-occurance\n",
    "for i in range(dataset.shape[0]):\n",
    "    # for each trigram pair\n",
    "    for w1, w2, w3 in create_traigram(dataset['Senteces'][i]):\n",
    "        # count the occurances of word3, given word 1 and word2\n",
    "        model[(we, w2)][w3] +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc6c2a-ebaa-417b-a704-1b621f07ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2981299-2ada-4a60-a75f-ae614522a1fd",
   "metadata": {},
   "source": [
    "### 4. Predicting the next word using N-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8202b-ebda-4d91-8db5-9189eef163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the next word\n",
    "dict(model['to', 'book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1644b9cb-8a59-4e8d-a039-affaa008105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another examples\n",
    "dict(model['my', 'name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024438f8-f2cf-4d69-bf95-7356de465bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model['how', 'are'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e3aa9-fe25-4dca-aed2-68c080a8e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model['good', 'to'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dbb7e-56c1-4a10-b6a8-0bba306b52b8",
   "metadata": {},
   "source": [
    "### Probabilistic Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4b334-7b73-4177-bc2b-006a4c027036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the unigram List\n",
    "unigram_list=[]\n",
    "for i in tqdm(dataset.shape[0]):\n",
    "    # add word -count pair to be dictionary\n",
    "    for word in dataset['unigram'][i]:\n",
    "        # check if the word is already in dictionary\n",
    "        if word[0] in unigram_dict:\n",
    "            # Increment count of word by 1\n",
    "            unigram_dict[word[0]= unigram_dict[word[0]] + 1\n",
    "        else:\n",
    "            # add the word to dictionary with count 1\n",
    "            unigram_dict[word[0]]= 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161be0d-7029-4143-b7f2-26f2e8dc8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd4875-ab39-449c-8739-2448d8df1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the overall frequ of words in the corpus\n",
    "counts= Counter(unigram_dict)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c6088-240c-4302-abb4-654d8776b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size\n",
    "total_count= len(unigram_dict)\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e0044-72ec-4e18-a818-1c91dc763256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative frqu of eachword\n",
    "for word in counts:\n",
    "    counts[word]/=float(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d9ae5-cf3e-4371-8d9a-5c9b90215f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e18a9e-9244-4da7-8377-6d7dbda5763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let transform the counts to probabilites\n",
    "for w1_w2 in model:\n",
    "    tota_count= float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] / = total_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf49bae-d040-4616-8f0d-6248e7992f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the next word\n",
    "dict(model['to', 'book'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85c754-a1b3-4dc6-aff2-da3eb1cdc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model['how', 'are'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
